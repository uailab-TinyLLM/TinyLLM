# TinyLLM

### Models
| Model Name | Explanation | ðŸ”— Link |
| --- | --- | --- |
|... |... |... |

### Evaluation Datasets

| Dataset Name | Explanation | ðŸ”— Link |
| --- | --- | --- |
| ARC | Science question dataset for QA.<br>- ARC-e : ARC-easy | [ai2_arc](https://huggingface.co/datasets/allenai/ai2_arc) |
| OBQA | a QA dataset modeled after open-book exams, designed to test multi-step reasoning, commonsense knowledge, and deep text comprehension. | [openbookqa](https://huggingface.co/datasets/allenai/openbookqa) |
| BoolQ | QA dataset for yes/no questions | [boolq](https://huggingface.co/datasets/google/boolq) |
| PIQA | QA dataset for physical commonsense reasoning and a corresponding | [piqa](https://huggingface.co/datasets/ybisk/piqa) |
| SIQA | question-answering,  designed to evaluate social commonsense reasoning about people's actions and their social implications. | [social_i_qa](https://huggingface.co/datasets/allenai/social_i_qa) |
| WinoGrande | fill-in-the-blank problems | [winogrande](https://huggingface.co/datasets/allenai/winogrande) |
| HellaSwag | Common sense natural language reasoning | [hellaswag](https://huggingface.co/datasets/Rowan/hellaswag) |
